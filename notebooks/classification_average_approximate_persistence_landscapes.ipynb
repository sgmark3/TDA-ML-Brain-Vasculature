{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import subprocess\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripser import ripser\n",
    "from persim.landscapes import (\n",
    "PersLandscapeApprox,\n",
    "average_approx,\n",
    "snap_pl,\n",
    "plot_landscape,\n",
    "plot_landscape_simple\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The persistence data are computed and stored in data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_in_str = '../data/annotated_PH_2018_500/'\n",
    "directory = os.fsencode(directory_in_str)\n",
    "\n",
    "folders_region1 = []\n",
    "\n",
    "for folder in os.listdir(directory):\n",
    "    foldername = os.fsdecode(folder)\n",
    "    if foldername.startswith('PH_640_'):\n",
    "        folders_region1.append(foldername)\n",
    "        foldername = foldername\n",
    "\n",
    "folders_region2 = []\n",
    "\n",
    "for folder in os.listdir(directory):\n",
    "    foldername = os.fsdecode(folder)\n",
    "    if foldername.startswith('PH_3_'):\n",
    "        folders_region2.append(foldername)\n",
    "        foldername = foldername"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get persistence data stored in files within the folder whose location is passed through the keyword argument path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_edges_threshold = 0\n",
    "\n",
    "\"\"\"\n",
    "Returns persistence data\n",
    "Inputs -- path: local_path\n",
    "       -- starts_with: the string that the filenames start with if needed\n",
    "       -- ends_with: the string that the filenames end with if needed\n",
    "       -- threshold: number of edges threshold of the underlying vascular networks\n",
    "\"\"\"\n",
    "\n",
    "def get_data(path=None,starts_with=None,ends_with=None,threshold=None):\n",
    "    files = [os.fsdecode(file) for file in os.listdir(path)]\n",
    "    persistence_data={}\n",
    "    for filename in files:\n",
    "        if starts_with and ends_with:\n",
    "            if filename.startswith(starts_with) and filename.endswith(ends_with):\n",
    "                data = []\n",
    "                with open(path+filename) as f:\n",
    "                    reader = csv.reader(f)\n",
    "                    for row in reader:\n",
    "                        data.append([float(x) for x in row])\n",
    "                if threshold == 0:\n",
    "                    persistence_data[filename]=[item for item in data[1:] if item[1]!=np.inf]\n",
    "                else:\n",
    "                    if data[0][1] >= threshold:\n",
    "                        persistence_data[filename]=[item for item in data[1:] if item[1]!=np.inf]\n",
    "        elif starts_with:\n",
    "            if filename.startswith(starts_with):\n",
    "                data = []\n",
    "                with open(path+filename) as f:\n",
    "                    reader = csv.reader(f)\n",
    "                    for row in reader:\n",
    "                        data.append([float(x) for x in row])\n",
    "                if threshold == 0:\n",
    "                    persistence_data[filename]=[item for item in data[1:] if item[1]!=np.inf]\n",
    "                else:\n",
    "                    if data[0][1] >= threshold:\n",
    "                        persistence_data[filename]=[item for item in data[1:] if item[1]!=np.inf]\n",
    "        elif ends_with:\n",
    "            if filename.endswith(ends_with):\n",
    "                data = []\n",
    "                with open(path+filename) as f:\n",
    "                    reader = csv.reader(f)\n",
    "                    for row in reader:\n",
    "                        data.append([float(x) for x in row])\n",
    "                if threshold == 0:\n",
    "                    persistence_data[filename]=[item for item in data[1:] if item[1]!=np.inf]\n",
    "                else:\n",
    "                    if data[0][1] >= threshold:\n",
    "                        persistence_data[filename]=[item for item in data[1:] if item[1]!=np.inf]\n",
    "                \n",
    "    return persistence_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For comparitive analysis between vascular networks in bulk of mouse brain tissue -- region-1 vs. region-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_PHs={}\n",
    "for folder in folders_region1:\n",
    "    r1_PHs[folder] = get_data('../data/annotated_PH_2018_500/'+folder+'/',starts_with=('ph_'),threshold=number_of_edges_threshold)\n",
    "\n",
    "r2_PHs={}\n",
    "for folder in folders_region2:\n",
    "    r2_PHs[folder] = get_data('../data/annotated_PH_2018_500/'+folder+'/',starts_with=('ph_'),threshold=number_of_edges_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the following helper function to get persistence data, which is stored within a dictionary of dictinoaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input -- persistence_data: a dictionary of folders consisting of dictionaries \n",
    "         of files whose values are the lists of persistence (b,d) pairs dataset\n",
    "\"\"\"\n",
    "\n",
    "def get_pl(persistence_data):\n",
    "    pl_data = []\n",
    "    for key,value in persistence_data.items():\n",
    "        if value:\n",
    "            for subkey,subvalue in value.items():\n",
    "                pl_data.append(subvalue)\n",
    "                if subvalue == []:\n",
    "                    print(subkey)\n",
    "    return pl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the persistence data, downsampling (optional) and creating the labels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6739130434782609, 322)\n"
     ]
    }
   ],
   "source": [
    "r1_ph_dataset = get_pl(r1_PHs)\n",
    "r2_ph_dataset = get_pl(r2_PHs)\n",
    "\n",
    "## Downsampling\n",
    "# indices = np.random.randint(0,len(r2_ph_dataset),len(r1_ph_dataset))\n",
    "# r2_ph_dataset = [r2_ph_dataset[k] for k in indices]\n",
    "\n",
    "y=[]\n",
    "for k in range(len(r1_ph_dataset)+len(r2_ph_dataset)):\n",
    "    if k < len(r1_ph_dataset):\n",
    "        y.append(0)\n",
    "    elif k >= len(r1_ph_dataset):\n",
    "        y.append(1)\n",
    "        \n",
    "dataset = [np.array(item) for item in r1_ph_dataset+r2_ph_dataset]\n",
    "\n",
    "## Check that the number of samples is consistent depending on whether or not the downsampling is applied\n",
    "print((sum(y)/len(y),len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "## for balanced dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "## for imbalanced dataset\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "## while doing cross-validation, this is optional\n",
    "ph_train, ph_test, y_train, y_test = train_test_split(dataset, y,\n",
    "                                                      test_size = 0.1,\n",
    "                                                      random_state=123,\n",
    "                                                      shuffle=True,\n",
    "                                                      stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute approimate persistence landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_pl_train_dataset=[]\n",
    "for entry in ph_train:\n",
    "    approx_pl_train_dataset.append(PersLandscapeApprox(dgms=[np.array([[item[0],item[1]] for item in entry])],hom_deg=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_pl_test_dataset=[]\n",
    "for entry in ph_test:\n",
    "    approx_pl_test_dataset.append(PersLandscapeApprox(dgms=[np.array([[item[0],item[1]] for item in entry])],hom_deg=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the average of the approximate persistence landscapes for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pl_train_region2 = average_approx([approx_pl_train_dataset[k] for k in np.squeeze(np.argwhere(y_train))])\n",
    "avg_pl_train_region1 = average_approx([approx_pl_train_dataset[j] for j in \\\n",
    "                                       [k for k in range(len(y_train)) if k not in np.squeeze(np.argwhere(y_train))]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the distance between the average approximate persistence landscape for the vascular networks in two regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sup-norm of the difference between BS and CH: 0.00975169228356082\n",
      "l1 norm of the difference between BS and CH: 0.06497366310909727\n",
      "l2 norm of the difference between BS and CH: 0.017962061813394953\n"
     ]
    }
   ],
   "source": [
    "[avg_pl_train_region1_snapped, avg_pl_train_region2_snapped] = snap_pl([avg_pl_train_region1, avg_pl_train_region2])\n",
    "print(f'Sup-norm of the difference between BS and CH: {(avg_pl_train_region1_snapped-avg_pl_train_region2_snapped).sup_norm()}')\n",
    "print(f'l1 norm of the difference between BS and CH: {(avg_pl_train_region1_snapped-avg_pl_train_region2_snapped).p_norm(1)}')\n",
    "print(f'l2 norm of the difference between BS and CH: {(avg_pl_train_region1_snapped-avg_pl_train_region2_snapped).p_norm(2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the distances from the averages of approximate persistence landscapes to classify samples from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_distance = np.zeros((len(approx_pl_test_dataset),2))\n",
    "l1_distance = np.zeros((len(approx_pl_test_dataset),2))\n",
    "l2_distance = np.zeros((len(approx_pl_test_dataset),2))\n",
    "for k,sample in enumerate(approx_pl_test_dataset):\n",
    "    [avg_pl_train_snapped_region1, test]=snap_pl([avg_pl_train_region1, sample])\n",
    "    t1=avg_pl_train_snapped_region1 - test\n",
    "    [avg_pl_train_snapped_region2, test]=snap_pl([avg_pl_train_region2, sample])\n",
    "    t2=avg_pl_train_snapped_region2 - test\n",
    "    sup_distance[k,0] = t1.sup_norm()\n",
    "    sup_distance[k,1] = t2.sup_norm()\n",
    "    l1_distance[k,0] = t1.p_norm(1)\n",
    "    l1_distance[k,1] = t2.p_norm(1)\n",
    "    l2_distance[k,0] = t1.p_norm(2)\n",
    "    l2_distance[k,1] = t2.p_norm(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The f1-scores for three different choice of norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5454545454545455, 0.6486486486486486, 0.7368421052631579)\n"
     ]
    }
   ],
   "source": [
    "print((f1_score(y_test,np.argmin(l1_distance,axis=1)),\\\n",
    "       f1_score(y_test,np.argmin(l2_distance,axis=1)),\\\n",
    "       f1_score(y_test,np.argmin(sup_distance,axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
